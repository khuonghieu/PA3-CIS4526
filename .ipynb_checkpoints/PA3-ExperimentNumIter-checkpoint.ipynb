{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(train_y, pred_y):\n",
    "    innerProduct = np.multiply(train_y, pred_y)\n",
    "    log_loss = np.log(1 + np.exp(-innerProduct))\n",
    "    return np.mean(log_loss)\n",
    "\n",
    "\n",
    "def hinge_loss(train_y, pred_y):\n",
    "    innerProduct = np.multiply(train_y, pred_y)\n",
    "    hinge_loss_vector = np.maximum(0,1-innerProduct)\n",
    "    return np.mean(hinge_loss_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of all the values of the weight\n",
    "def l1_reg(w):\n",
    "    l1_reg_loss = 0;\n",
    "    for i in range(1,len(w)):\n",
    "        l1_reg_loss += abs(w[i])\n",
    "    return l1_reg_loss\n",
    "\n",
    "#Dot product of the weight with itself\n",
    "def l2_reg(w):    \n",
    "    l2_reg_loss = np.dot(w[1:], np.transpose(w[1:]))\n",
    "    return l2_reg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(train_x, train_y, learn_rate, loss, lambda_val=None, regularizer=None):\n",
    "    #Initialize weight with bias\n",
    "    weight_vector = np.random.rand(len(train_x[0]) + 1) \n",
    "    #Iteration\n",
    "    num_iters = 100\n",
    "    #Numerical_differentiation, as suggested\n",
    "    h = 0.0001\n",
    "    #Run for num_iters times\n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        current_weight = np.copy(weight_vector)\n",
    "        #Delta_weight for updating: w = w - learn_rate*delta_weight\n",
    "        delta_weight = np.zeros(len(train_x[0]) + 1) \n",
    "        #Prediction using current weight\n",
    "        predict_y = test_classifier(current_weight,train_x)\n",
    "        \n",
    "        #Check for lambda existence, if yes then add regularization\n",
    "        if(lambda_val):\n",
    "            current_loss = loss(train_y, predict_y) + lambda_val*regularizer(current_weight)\n",
    "        else:\n",
    "            current_loss = loss(train_y, predict_y)\n",
    "            \n",
    "        \n",
    "        for i in range(len(delta_weight)):\n",
    "            tmp_current_weight = np.copy(current_weight)\n",
    "            tmp_current_weight[i] = tmp_current_weight[i] + h;\n",
    "            \n",
    "            tmp_predict_y = test_classifier(tmp_current_weight,train_x)\n",
    "            \n",
    "            # Find delta_weight using loss function\n",
    "            \n",
    "            #Check for lambda existence\n",
    "            if(lambda_val):\n",
    "                tmp_loss = loss(train_y, tmp_predict_y) + lambda_val*regularizer(tmp_current_weight)\n",
    "            else:\n",
    "                tmp_loss = loss(train_y, tmp_predict_y)\n",
    "            \n",
    "            #Differentiation\n",
    "            delta_weight[i] = (tmp_loss - current_loss) / h\n",
    "\n",
    "        #Update weight\n",
    "        weight_vector = current_weight - learn_rate*delta_weight\n",
    "           \n",
    "    return weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return pred_y as inner product of weights and feature values\n",
    "def test_classifier(w, test_x):\n",
    "    pred_y = np.zeros(len(test_x))\n",
    "    for i in range(len(test_x)):\n",
    "        pred_y[i] = np.dot(w[1:], test_x[i]) + w[0]\n",
    "    return pred_y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(trainX,testX):\n",
    "    # Standardize the dataset\n",
    "    dataX_trans = trainX.transpose()\n",
    "    column = 0\n",
    "    for row in dataX_trans:\n",
    "        #Subtract mean from every value, then divide by deviation\n",
    "        mean = np.mean(row)\n",
    "        std = np.std(row)\n",
    "        for i in range(len(dataX_trans[0])):\n",
    "            trainX[i][column] -= mean\n",
    "            trainX[i][column] /= std\n",
    "        for i in range(len(testX.transpose()[0])):\n",
    "            testX[i][column] -= mean\n",
    "            testX[i][column] /= std\n",
    "        column += 1\n",
    "\n",
    "#Find accuracy\n",
    "def compute_accuracy(test_y, pred_y):\n",
    "    #Convert predicted label into -1 and 1\n",
    "    convert_pred_y = np.copy(pred_y)\n",
    "    for j in range(len(convert_pred_y)):\n",
    "        if(convert_pred_y[j] < 6):\n",
    "            convert_pred_y[j] = -1\n",
    "        elif (convert_pred_y[j] > 6):\n",
    "            convert_pred_y[j] = 1\n",
    "    #Vector filled with booleans\n",
    "    compare = (test_y == convert_pred_y)\n",
    "    match_count = 0\n",
    "    for i in range(len(compare)):\n",
    "        if (compare[i] == True):\n",
    "            match_count += 1 \n",
    "    \n",
    "    return (match_count/len(test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmargin SVM ,learn rate: 0.1 ,lambda: 0.1\n",
      "Accuracy 0.6074074074074074\n",
      "Logistic Regression ,learn rate 0.1\n",
      "Accuracy 0.6074074074074074\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.1 ,lambda: 1\n",
      "Accuracy 0.6074074074074074\n",
      "Logistic Regression ,learn rate 0.1\n",
      "Accuracy 0.6074074074074074\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.1 ,lambda: 100\n",
      "Accuracy 0.5214814814814815\n",
      "Logistic Regression ,learn rate 0.1\n",
      "Accuracy 0.6074074074074074\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.1 ,lambda: 10\n",
      "Accuracy 0.37407407407407406\n",
      "Logistic Regression ,learn rate 0.1\n",
      "Accuracy 0.6129629629629629\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.1 ,lambda: 0.01\n",
      "Accuracy 0.6185185185185185\n",
      "Logistic Regression ,learn rate 0.1\n",
      "Accuracy 0.652962962962963\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.1 ,lambda: 0.001\n",
      "Accuracy 0.6774074074074073\n",
      "Logistic Regression ,learn rate 0.1\n",
      "Accuracy 0.6762962962962963\n",
      "\n",
      "Softmargin SVM ,learn rate: 1e-05 ,lambda: 0.1\n",
      "Accuracy 0.6388888888888888\n",
      "Logistic Regression ,learn rate 1e-05\n",
      "Accuracy 0.6255555555555555\n",
      "\n",
      "Softmargin SVM ,learn rate: 1e-05 ,lambda: 1\n",
      "Accuracy 0.5511111111111111\n",
      "Logistic Regression ,learn rate 1e-05\n",
      "Accuracy 0.6488888888888888\n",
      "\n",
      "Softmargin SVM ,learn rate: 1e-05 ,lambda: 100\n",
      "Accuracy 0.5377777777777777\n",
      "Logistic Regression ,learn rate 1e-05\n",
      "Accuracy 0.5666666666666667\n",
      "\n",
      "Softmargin SVM ,learn rate: 1e-05 ,lambda: 10\n",
      "Accuracy 0.5885185185185186\n",
      "Logistic Regression ,learn rate 1e-05\n",
      "Accuracy 0.5211111111111111\n",
      "\n",
      "Softmargin SVM ,learn rate: 1e-05 ,lambda: 0.01\n",
      "Accuracy 0.5055555555555555\n",
      "Logistic Regression ,learn rate 1e-05\n",
      "Accuracy 0.49777777777777776\n",
      "\n",
      "Softmargin SVM ,learn rate: 1e-05 ,lambda: 0.001\n",
      "Accuracy 0.5033333333333333\n",
      "Logistic Regression ,learn rate 1e-05\n",
      "Accuracy 0.5014814814814814\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.01 ,lambda: 0.1\n",
      "Accuracy 0.5037037037037037\n",
      "Logistic Regression ,learn rate 0.01\n",
      "Accuracy 0.4829629629629629\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.01 ,lambda: 1\n",
      "Accuracy 0.5081481481481482\n",
      "Logistic Regression ,learn rate 0.01\n",
      "Accuracy 0.5366666666666667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:183: RuntimeWarning: invalid value encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmargin SVM ,learn rate: 0.01 ,lambda: 100\n",
      "Accuracy 0.11962962962962961\n",
      "Logistic Regression ,learn rate 0.01\n",
      "Accuracy 0.12222222222222223\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.01 ,lambda: 10\n",
      "Accuracy 0.0\n",
      "Logistic Regression ,learn rate 0.01\n",
      "Accuracy 0.0\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.01 ,lambda: 0.01\n",
      "Accuracy 0.0\n",
      "Logistic Regression ,learn rate 0.01\n",
      "Accuracy 0.0\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.01 ,lambda: 0.001\n",
      "Accuracy 0.0\n",
      "Logistic Regression ,learn rate 0.01\n",
      "Accuracy 0.0\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.0001 ,lambda: 0.1\n",
      "Accuracy 0.0\n",
      "Logistic Regression ,learn rate 0.0001\n",
      "Accuracy 0.0\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.0001 ,lambda: 1\n",
      "Accuracy 0.0\n",
      "Logistic Regression ,learn rate 0.0001\n",
      "Accuracy 0.0\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.0001 ,lambda: 100\n",
      "Accuracy 0.0\n",
      "Logistic Regression ,learn rate 0.0001\n",
      "Accuracy 0.0\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.0001 ,lambda: 10\n",
      "Accuracy 0.0\n",
      "Logistic Regression ,learn rate 0.0001\n",
      "Accuracy 0.0\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.0001 ,lambda: 0.01\n",
      "Accuracy 0.0\n",
      "Logistic Regression ,learn rate 0.0001\n",
      "Accuracy 0.0\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.0001 ,lambda: 0.001\n",
      "Accuracy 0.0\n",
      "Logistic Regression ,learn rate 0.0001\n",
      "Accuracy 0.0\n",
      "\n",
      "Softmargin SVM ,learn rate: 0.001 ,lambda: 0.1\n",
      "Accuracy 0.0\n",
      "Logistic Regression ,learn rate 0.001\n",
      "Accuracy 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Read the training data file\n",
    "    szDatasetPath = 'winequality-white.csv'\n",
    "    listClasses = []\n",
    "    listAttrs = []\n",
    "    bFirstRow = True\n",
    "    with open(szDatasetPath) as csvFile:\n",
    "        csvReader = csv.reader(csvFile, delimiter=',')\n",
    "        for row in csvReader:\n",
    "            if bFirstRow:\n",
    "                bFirstRow = False\n",
    "                continue\n",
    "            if int(row[-1]) < 6:\n",
    "                listClasses.append(-1)\n",
    "                listAttrs.append(list(map(float, row[1:len(row) - 1])))\n",
    "            elif int(row[-1]) > 6:\n",
    "                listClasses.append(+1)\n",
    "                listAttrs.append(list(map(float, row[1:len(row) - 1])))\n",
    "    dataX = np.array(listAttrs)\n",
    "    dataY = np.array(listClasses)\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    np.set_printoptions(precision=8)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    subsetNum = int((len(dataX)/5))\n",
    "    \n",
    "    #Learn rate and lambda value to try out\n",
    "    learn_rate_array = {0.1,0.01,0.001,0.0001,0.00001}\n",
    "    lambda_array = {100,10,1,0.1,0.01,0.001}\n",
    "    for learnRate in learn_rate_array:\n",
    "        for lambdaVal in lambda_array:\n",
    "            #Fold 5 times, split the train and test sets\n",
    "            acc_svm_avg=0\n",
    "            acc_log_avg=0\n",
    "            #Initialize 5-folds\n",
    "            for i in range(5):\n",
    "                if(i == 0):\n",
    "                    #Split\n",
    "                    subdataX = np.split(dataX,[subsetNum])\n",
    "                    subdataY = np.split(dataY,[subsetNum])\n",
    "                    #Train\n",
    "                    trainX = subdataX[1]\n",
    "                    trainY = subdataY[1]\n",
    "                    #Test\n",
    "                    testX = subdataX[0]\n",
    "                    testY = subdataY[0]\n",
    "                elif(i == 4):\n",
    "                    #Split\n",
    "                    subdataX = np.split(dataX,[subsetNum*i])\n",
    "                    subdataY = np.split(dataY,[subsetNum*i])\n",
    "                    #Train\n",
    "                    trainX = subdataX[0]\n",
    "                    trainY = subdataY[0]\n",
    "                    #Test\n",
    "                    testX = subdataX[1]\n",
    "                    testY = subdataY[1]\n",
    "                else:\n",
    "                    #Split\n",
    "                    subdataX = np.split(dataX,[subsetNum*i,subsetNum*(i+1)])\n",
    "                    subdataY = np.split(dataY,[subsetNum*i,subsetNum*(i+1)])\n",
    "                    #Train\n",
    "                    trainX = np.concatenate((subdataX[0],subdataX[2]),axis=0)\n",
    "                    trainY = np.concatenate((subdataY[0],subdataY[2]),axis=0)\n",
    "                    #Test\n",
    "                    testX = subdataX[1]\n",
    "                    testY = subdataY[1]\n",
    "                #Normalize train and test sets\n",
    "                normalize(trainX,testX)\n",
    "                #Soft Margin SVM\n",
    "                weight_vector_svm = train_classifier(trainX,trainY,learnRate,hinge_loss,lambdaVal,l2_reg)\n",
    "                pred_y_svm = test_classifier(weight_vector_svm,testX)\n",
    "                acc_svm = compute_accuracy(testY,pred_y_svm)\n",
    "                acc_svm_avg+=acc_svm\n",
    "                #Logistic Regression\n",
    "                weight_vector_log = train_classifier(trainX,trainY,learnRate,logistic_loss)\n",
    "                pred_y_log = test_classifier(weight_vector_log,testX)\n",
    "                acc_log = compute_accuracy(testY,pred_y_log)\n",
    "                acc_log_avg+=acc_log\n",
    "            print(\"Softmargin SVM\",',learn rate:',learnRate,',lambda:',lambdaVal)\n",
    "            print(\"Accuracy\",acc_svm_avg/5)\n",
    "            print(\"Logistic Regression\",\",learn rate\",learnRate)\n",
    "            print(\"Accuracy\",acc_log_avg/5)\n",
    "            print()\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
